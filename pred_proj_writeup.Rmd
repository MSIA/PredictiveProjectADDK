---
title: "Predictive Analytics Project"
author: "Dylan Fontaine, Dustin Fontaine, Kristin Meier, and Aditya Venkataraman"
date: "November 19, 2016"
output: html_document
linestretch: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, echo=F}
source('/Users/arvenkat/Documents/PredictiveProjectADDK/R/models_km_inclNA.R')

#finalcode <- "/Users/arvenkat/Documents/PredictiveProjectADDK/"
#### RUN THE FUNCTION FROM THE CODE ABOVE SO IT CAN BE REFERENCED BELOW AND WE DONT NEED TO HARD CODE ANY NUMBERS
somefunction()
#### SPECIFY FINAL VERSION OF MODELS TO REFERENCE THE SUMMARY STATS
  # can have more things like this..depending on what our write up looks like!
  # or functions to be called later that have exhibits/tables
logistic_model <- x
mult_reg_model <- y

# http://rmarkdown.rstudio.com/pdf_document_format.html#table_of_contents
```

##1 Cover Page

Written Report
The text part of the report should be no more than 20 pages (double spaced, 12 point font). Put the
outputs, plots etc. in an appendix. The report should roughly follow the outline below.
1. Cover page (Title, names of group members)

Generic Title

Kristin Meier, Dustin Fontaine, Dylan Fontaine, Aditya Venkataraman


\pagebreak

# Executive Summary

Give a non-technical summary of your fndings mentioning the key predictors of responders vs. non-responders and of the amount of contributions. This summary is for upper management and should not include any equations and as few statistics as possible. So don't mention things like R2 here. (About 1/2 page)

\pagebreak

# Introduction

Describe your overall approach and any a priori hypotheses. Give a brief outline of the other sections of the report. (About 2 pages)


\pagebreak

#4 Model Fitting


##4.1 Data Pre-Processing

###4.1.1 Data Cleaning

Before any models were run, various tables needed to be merged and variables added. The initial tables were merged based on the "CN" and "SN" codes in order to get a lookup category for each of the codes. The table was joined with the region file so the regions of the donations could be accessed. 

Various variables were removed as they were redundant. All the code variables, id and date variables were removed. There was an apparent typo in the months since first contribution where 146 months since latest contribution was encoded as 1146 months (which was reverted back to the correct state). Wherever a second or third donation did not exist, those missing values were replaced with 0. 

The last decision was whether to filter out columns that included NAs. The dataset was duplicated so that NAs could be included or filtered out. Models were run under both conditions. 

###4.1.2 Added Calculated Variables
The model involved keeping only relevant columns including some additional added variables and dropping irrelevant variables. There were a few added variables based on calculations from the previous variables. The added variables included an average dollars donated calculation (avg) that consisted of dividing the total lifetime contribution divided by the amount of times contributed. In addition, the variable "average time" was calculated from the time since the first contribution to the latest contribution divided by the average lifetime contribution wherever defined (0 otherwise). Two dummy binary variables, don2 and don3, were created to to see if the donator had donated twice or thrice. Lastly, an interaction variable "increasing" to see if there was calculated to see if there was an increase from the second donation to the first donation, if available.

###4.1and a half EDA?

add to show how we decided to use quadratic terms and specific interactions...histograms, pairs plots, etc..


##4.2 Logistic Regression Model Overview

The main model used to classify whether a donater donated was a binary logistic regression model. The encoding was chosen such that 0 corresponds to a non-donator and 1 corresponds to a donator. This encoding was based on the variable target dollars since 1995 being greater than 0 for 1 and 0 for 0. Each of the logistic models that were used included various subsets, transformations and interactions of the features. The main metric used to evaluate perfomance was the AUC (area under curve) and the CCR.


###4.2.1 Basic Logistic Regression Model

The initial model fit all the terms that were included in the cleaned up data frame. The logistic model's AUC was `r I(auc(model=logModel))`  and the CCR was `r I(ccr(model=logModel))`

###4.2.2 Forwards and Backwards Logistic Regression Model

We will further filter variables which to use by using backward stepwise regression from our original model. This algorithm will include all variables and then remove variables subsequently. The backwards algorithm consisted of 'CNDOL1 + CNTRLIF + CONLARG + CONTRFST + CNDOL2 + CNTMLIF + 
    CNMON1 + CNMONF + CNMONL + avgTime + don2 + don3 + incr_don + 
    SEX + ContType1 + SolType1 + Region' (reformat). The AUC and CCR respectively was 
`r I(auc(model=backwards))` and `r I(auc(model= backwards))`

In addditon, the forwards algorithm was used starting with a null model and then adding terms iteratively. The forwards algorithm chose 'CNMON1 + CNMONL + CNTMLIF + CNMONF + ContType1 + CONTRFST + 
    SolType1 + incr_don + SEX + don2 + avgTime + CNTRLIF + CNDOL2 + 
    CNDOL1 + CONLARG + don3 + Region' as variables to keep in the logistic model. The AUC and CCR respectively was 
`r I(auc(model=forwards))` and `r I(auc(model= forwards))` 


###4.2.3 Quadratic Terms Logistic Regression Model

Based on the exploratory data analysis, terms that involved squaring the predictors were included in addition to the original predictors (expand). The AUC and CCR for quadratic terms was `r I(auc(model=logModel.quad, testdata = donTEST.quad))` and 
`r I(ccr(model=logModel.quad, testdata = donTEST.quad))`.

Backwards stepwise regression were performed again to yield the ideal number of predictors. The predictors chosen for back regression with quadratic terms were "sq_CNDOL1 + sq_CONLARG + sq_CNDOL2 + sq_CNTMLIF + sq_CNMON1 + 
    sq_CNMONL + sq_avg + sq_avgTime + CNDOL1 + CNTRLIF + CONLARG + 
    CONTRFST + CNDOL2 + CNTMLIF + CNMON1 + CNMONF + CNMONL + 
    avg + avgTime + don2 + don3 + incr_don + SEX + ContType1 + 
    SolType1 + Region".
    
The AUC and CCR respectively were `r I(auc(model=backwards.quad, testdata = donTEST.quad))` and `r I(ccr(model=backwards.quad, testdata = donTEST.quad))`.

###4.2.4 Interactions Included Logistic Regression

The interactions that were considered were the largest contribution, all the latest contributions and lifetime contribution against all predictors. The auc and ccr respectively were `r I(auc(model=logModelInter, testdata=donTEST2, testresponse = donTEST2$donated))` and `r I(ccr(model=logModelInter, testdata=donTEST2, testresponse = donTEST2$donated))`

###4.2.5 Information Gain Model?

###4.2.6 GLM (if time)?

###4.3 Multiple Regression Model

Add- Same methodology as above but using multiple linear regression


##4.4 Model Diagonsitics and Final Model Chosen

Add- Graphs, QQnorms, VIF/Cooks distance etc

\pagebreak

#5 Model Validation

Explain how you validated the model against the test data set. Report the results about how well the model predicted the test set sales values and how well your top 1,000 predicted customers performed in terms of contributions. (About 4 pages)



\pagebreak

# Conclusions

Draw conclusions about significant predictors, any key missing predictors which
would have improved the model, etc. (About 2 pages)

\pagebreak

# References

\pagebreak

# Appendix (Printouts, Graphs)

```{r, echo=F}

```
